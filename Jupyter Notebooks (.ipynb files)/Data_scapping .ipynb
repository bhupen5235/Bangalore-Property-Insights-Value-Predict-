{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing required liberaries\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import pandas as pd\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import requests\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Declaring link of website \n",
    "link=\"https://www.makaan.com/\"\n",
    "requests.get(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = Service(r\"C:\\Users\\bhupe\\Downloads\\chromedriver-win32\\chromedriver-win32\\chromedriver.exe\") # location of Web driver\n",
    "driver = webdriver.Chrome(service=s) #Intialising Web driver\n",
    "driver.maximize_window() \n",
    "driver.get(link)\n",
    "city=\"Bangalore\"  ##-----Put name of city for which you want to scrap data( check spellings) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of total pages available on wesite for Bangalore:-->2176\n"
     ]
    }
   ],
   "source": [
    "# Finding *Serach Bar* Path in website and Pushing city as \"Bangalore\"\n",
    "search_bar_xpath = \"/html/body/div[1]/main/div/section[1]/div[1]/div/div[3]/div[1]/div/div/div/div[2]/input\"\n",
    "search_bar = driver.find_element(By.XPATH, search_bar_xpath)\n",
    "search_bar.send_keys(city) \n",
    "\n",
    "\n",
    "# Wait for the \"result-row\" element to be clickable and then click it\n",
    "result_row = WebDriverWait(driver, 5).until(EC.element_to_be_clickable((By.CLASS_NAME,\"result-row\")))\n",
    "result_row.click()\n",
    "\n",
    "\n",
    "\n",
    "# Finding  *Search Button* elment and then click on it\n",
    "search_button_xpath = \"/html/body/div[1]/main/div/section[1]/div[1]/div/div[3]/div[3]/span[2]\"\n",
    "search_button = WebDriverWait(driver, 5).until(EC.visibility_of_element_located((By.XPATH, search_button_xpath)))\n",
    "search_button.click()\n",
    "\n",
    "\n",
    "driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\") # Scroll to bottom of page\n",
    "\n",
    "\n",
    "\n",
    "# Wait for the \"page numbers\" element to be visible and then extract the number of pages\n",
    "page_numbers_xpath = \"/html/body/div[1]/main/div/div/div[2]/div/div[2]/div/div[3]/div[1]/div/ul/li[7]/a\"\n",
    "page_numbers_element = WebDriverWait(driver, 5).until(EC.visibility_of_element_located((By.XPATH, page_numbers_xpath)))\n",
    "total_page = int(page_numbers_element.text)\n",
    "print(f'No of total pages available on wesite for {city}:-->{total_page}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic={\"property\":[],\"location\":[],\"price\":[],\"area\":[],\"furnished status\":[],\"additional details\":[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining function to Scrapp data of Makaan website \n",
    "def Scrap_Makaan(pages):    \n",
    "    for page in range(1,pages):\n",
    "        if page % 50 == 0:\n",
    "            time.sleep(30)\n",
    "\n",
    "        driver.get(f\"https://www.makaan.com/bangalore-residential-property/buy-property-in-bangalore-city?page={page}&_=1725726991020\")\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        \n",
    "        listings = driver.find_elements(By.CLASS_NAME, \"cardholder\")  \n",
    "\n",
    "        for listing in listings:\n",
    "            try:\n",
    "                location = listing.find_element(By.CLASS_NAME, \"loclink\").text\n",
    "            except:\n",
    "                location = \"NULL\"\n",
    "            \n",
    "            try:\n",
    "                property_name = listing.find_element(By.CLASS_NAME, \"title-line\").text\n",
    "            except:\n",
    "                property_name = \"NULL\"\n",
    "            \n",
    "            try:\n",
    "                price = listing.find_element(By.CLASS_NAME, \"price\").text\n",
    "            except:\n",
    "                price = \"NULL\"\n",
    "            \n",
    "            try:\n",
    "                area = listing.find_element(By.CLASS_NAME, \"size\").text\n",
    "            except:\n",
    "                area = \"NULL\"\n",
    "            \n",
    "            try:\n",
    "                furnished_status = listing.find_element(By.CLASS_NAME, \"hcol.w44\").text\n",
    "            except:\n",
    "                furnished_status = \"NULL\"\n",
    "            \n",
    "            try:\n",
    "                additional_details = listing.find_element(By.CLASS_NAME, \"listing-details\").text\n",
    "            except:\n",
    "                additional_details = \"NULL\"\n",
    "\n",
    "            # Append all values to the dictionary\n",
    "            dic[\"location\"].append(location)\n",
    "            dic[\"property\"].append(property_name)\n",
    "            dic[\"price\"].append(price)\n",
    "            dic[\"area\"].append(area)\n",
    "            dic[\"furnished status\"].append(furnished_status)\n",
    "            dic[\"additional details\"].append(additional_details)\n",
    "\n",
    "        # Save to CSV when the data reaches 5000 entries\n",
    "            if len(dic[\"location\"]) == 5000:\n",
    "                i=1\n",
    "                file = pd.DataFrame(dic)\n",
    "                print(f'-----------FILE ::: {i}--{5000*i} <-- Data Extracted')\n",
    "                print(file.head())\n",
    "                file.to_csv(f\"makan{i}.csv\", index=False)\n",
    "                dic = {\"property\": [], \"location\": [], \"price\": [], \"area\": [], \"furnished status\": [], \"additional details\": []}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling function to Scrap Data\n",
    "Scrap_Makaan(total_page)  #pass the number of pages you want to scrap"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
